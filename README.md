# YouTube-Summarizer ğŸ“¼â†’ğŸ“

A lightweight research utility that

1. **fetches** closed-caption transcripts for any public YouTube video
2. **summarises** them locally with an **Ollama-hosted LLM** (no cloud tokens)
3. **stores** the results as Markdown â€œcontext docsâ€ and an audit log for later use.

Itâ€™s a self-contained side projectâ€”drop the generated corpus into Guild _when youâ€™re ready_, or keep it separate for ad-hoc analysis.

---

## âœ¨ Key Features

| Capability       | Detail                                                                                                      |
| ---------------- | ----------------------------------------------------------------------------------------------------------- |
| Transcript fetch | Uses [`youtube-transcript-api`](https://pypi.org/project/youtube-transcript-api/) â€“ no Selenium, no API key |
| Local LLM        | Any model Ollama can serve (LlamaÂ 3, Phiâ€‘3, Mistral, etc.)                                                  |
| Chunking         | Splits long videos into â‰¤ _N_ tokens for highâ€‘quality summaries                                             |
| Outputs          | _data/raw/_.json (cache)Â â€¢Â _data/corpus/_.md (summaries)Â â€¢Â _logs/ingest.jsonl_                              |
| CLI-first        | `yt-summarizer ids.txt --model llama3:8b`                                                                   |

---

## ğŸ”§ Requirements

- PythonÂ â‰¥â€¯3.11Â <â€¯3.14
- **Poetry** for dependency/venv management
- **Ollama** runtime (runs on CPU or GPU)

> **No external APIs** â€“ everything runs locally once transcripts are cached.

---

## ğŸ Quick Start

```bash
# clone wherever you like
git clone https://github.com/your-user/youtube-summarizer.git
cd youtube-summarizer

# create venv + install deps
poetry install

# (recommended) let direnv auto-activate the venv
direnv allow         # if .envrc is present

# pull an Ollama model (first time only)
ollama pull llama3:8b

# fire up the Ollama server in a separate terminal
ollama serve         # listens on  http://localhost:11434

# run the pipeline on a list of video IDs / URLs
echo "dQw4w9WgXcQ" > ids.txt
poetry run yt-summarizer ids.txt --model llama3:8b
```

Results:

```bash
âœ“ dQw4w9WgXcQ: 3 chunks summarised
data/
 â”œâ”€ raw/dQw4w9WgXcQ.json         # full transcript
 â””â”€ corpus/dQw4w9WgXcQ.md        # nicely formatted summary
logs/ingest.jsonl               # run log
```

---

## ğŸ–¥ï¸ Using Ollama

| Step                   | Command                                                                            | Notes                                           |
| ---------------------- | ---------------------------------------------------------------------------------- | ----------------------------------------------- | -------------------------------------------- |
| **Install** Ollama     | macOS: `brew install ollama`<br>Linux: `curl -fsSL https://ollama.ai/install.sh    | sh`                                             | See <https://ollama.ai> for other platforms. |
| **Pull** a model image | `ollama pull phi3:mini`                                                            | Downloads once; stored in `~/.ollama`           |
| **Serve** locally      | `ollama serve`                                                                     | Runs HTTP API on **11434** until you stop it.   |
| **Switch models**      | Add `--model <tag>` flag when you run `yt-summarizer`, e.g.<br>`--model phi3:mini` | Any tag shown by `ollama list` works.           |
| **Custom default**     | `export OLLAMA_DEFAULT_MODEL="phi3:mini"` before `ollama serve`                    | Requests that omit `"model"` will use this tag. |

> Memory-bound? Use a 7â€‘8â€¯billionâ€‘param model (e.g. `phi3:mini`, `mistral:7b-instruct`) or run on CPU with `OLLAMA_NO_GPU=1`.

---

## âš™ï¸ CLI Reference

```bash
yt-summarizer IDS_FILE [options]

Positional:
  IDS_FILE          Text file with one YouTube ID or URL per line

Options:
  --model TAG       Ollama model tag   [default: llama3:8b]
  --chunk-size N    Max tokens per chunk before summarising (2048 default)
  --help            Show this help
```

_The script automatically caches transcripts; re-running on the same IDs costs only LLM time._

---

## ğŸ“‚ Data Layout

```
data/
 â”œâ”€ raw/                  # untouched transcripts (.json)
 â””â”€ corpus/               # Markdown summaries
logs/
 â””â”€ ingest.jsonl          # one JSON object per video processed
```

The Markdown front-matter includes:

```yaml
---
video_id: dQw4â€¦
url: https://youtu.be/dQw4â€¦
saved: 2025-05-22T02:17:45Z
tags: [youtube, transcript]
---
```

Guildâ€™s Corpus loader can ingest these without changes when youâ€™re ready.

---

## ğŸ› ï¸ Troubleshooting

| Symptom                                             | Fix                                                                      |
| --------------------------------------------------- | ------------------------------------------------------------------------ |
| `NoTranscriptFound`                                 | Video has no public captions; nothing we can do.                         |
| `HTTP 404` / `failed to fetch manifest` from Ollama | Mistyped model tag â€“ run `ollama list` or `ollama pull <tag>`.           |
| Out-of-memory crash                                 | Use a smaller model (`phi3:mini`, `llama3:8b`) or set `OLLAMA_NO_GPU=1`. |
| Poetry solver complains about PythonÂ 3.14           | Project pins Python to `<3.14`; use 3.11â€‘3.13.                           |

---

## ğŸ“ License

MIT Â© Lance Rogers  
Contributions welcomeâ€”open a PR or discussion!
